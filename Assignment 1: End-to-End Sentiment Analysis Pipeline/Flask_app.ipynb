{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install pyngrok\n",
        "!pip install flask"
      ],
      "metadata": {
        "id": "YJ0-7mFBPH4b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "trAjxaXPEccK"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import torch\n",
        "from transformers import AutoTokenizer, BertForSequenceClassification\n",
        "from flask import Flask, request, jsonify\n",
        "import os\n",
        "import requests\n",
        "from pyngrok import ngrok\n",
        "# from flask_ngrok import run_with_ngrok"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ngrok.set_auth_token(\"2m3INDfD7mYEcHw8VB1STZuofFc_UBjx4mHCjjpu5iZbbxgN\")"
      ],
      "metadata": {
        "id": "wR6uyJURNlVK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "app = Flask(__name__)\n",
        "\n",
        "\n",
        "MODEL = None\n",
        "TOKENIZER = None\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"\n",
        "    Preprocess text using the same steps as training\n",
        "    \"\"\"\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"<.*?>\", \"\", text)  # Remove HTML tags\n",
        "    text = re.sub(r\"[^\\w\\s]\", \"\", text)  # Remove punctuation\n",
        "\n",
        "    return text\n",
        "\n",
        "def load_model(model_path):\n",
        "    \"\"\"\n",
        "    Load the saved model and tokenizer\n",
        "    \"\"\"\n",
        "    global MODEL, TOKENIZER\n",
        "\n",
        "    # Load tokenizer from pretrained BERT\n",
        "    TOKENIZER = AutoTokenizer.from_pretrained(\"textattack/bert-base-uncased-yelp-polarity\")\n",
        "\n",
        "    # Load the fine-tuned model\n",
        "    MODEL = BertForSequenceClassification.from_pretrained(model_path)\n",
        "    MODEL.to(DEVICE)\n",
        "    MODEL.eval()\n",
        "\n",
        "    print(\"Model loaded successfully!\")\n",
        "\n",
        "def predict_sentiment(text):\n",
        "    \"\"\"\n",
        "    Predict sentiment for given text\n",
        "    \"\"\"\n",
        "    # Clean the text first\n",
        "    cleaned_text = clean_text(text)\n",
        "\n",
        "    # Tokenize the cleaned text\n",
        "    encoding = TOKENIZER(\n",
        "        cleaned_text,\n",
        "        truncation=True,\n",
        "        padding='max_length',\n",
        "        max_length=512,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "\n",
        "    # Move inputs to device\n",
        "    input_ids = encoding['input_ids'].to(DEVICE)\n",
        "    attention_mask = encoding['attention_mask'].to(DEVICE)\n",
        "\n",
        "    # Get prediction\n",
        "    with torch.no_grad():\n",
        "        outputs = MODEL(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        prediction = torch.argmax(outputs.logits, dim=1)\n",
        "\n",
        "    # Convert prediction to label\n",
        "    sentiment = \"positive\" if prediction.item() == 1 else \"negative\"\n",
        "\n",
        "    return sentiment\n",
        "\n",
        "@app.route('/predict', methods=['POST'])\n",
        "def predict():\n",
        "    \"\"\"\n",
        "    Endpoint for sentiment prediction\n",
        "    \"\"\"\n",
        "    # Check if request contains JSON\n",
        "    if not request.is_json:\n",
        "        return jsonify({\"error\": \"Request must be JSON\"}), 400\n",
        "\n",
        "    # Get the text from request\n",
        "    data = request.get_json()\n",
        "    if 'review_text' not in data:\n",
        "        return jsonify({\"error\": \"Missing review_text field\"}), 400\n",
        "\n",
        "    review_text = data['review_text']\n",
        "\n",
        "    try:\n",
        "        # Get prediction with cleaned text\n",
        "        sentiment = predict_sentiment(review_text)\n",
        "\n",
        "        # Return prediction\n",
        "        return jsonify({\n",
        "            \"sentiment_prediction\": sentiment\n",
        "        })\n",
        "\n",
        "    except Exception as e:\n",
        "        return jsonify({\"error\": str(e)}), 500\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Load model when starting the server\n",
        "    model_path = \"/content/drive/MyDrive/Assignment data science intern/Finetuned model\"  # Replace with your model path\n",
        "    load_model(model_path)\n",
        "    ngrok.connect(5000)\n",
        "\n",
        "    # Get the public URL\n",
        "    tunnels = ngrok.get_tunnels()\n",
        "    ngrok_url = tunnels[0].public_url\n",
        "    print(f\" * Public URL: {ngrok_url}\")\n",
        "\n",
        "    # Run Flask app\n",
        "    app.run(port=5000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XVW5nNbLkImq",
        "outputId": "98167d5c-041e-43d6-daf2-075b59417725"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded successfully!\n",
            " * Public URL: https://bd36-34-121-3-207.ngrok-free.app\n",
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on http://127.0.0.1:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n",
            "INFO:werkzeug:127.0.0.1 - - [28/Jan/2025 11:18:12] \"POST /predict HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [28/Jan/2025 11:18:14] \"POST /predict HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [28/Jan/2025 11:21:32] \"POST /predict HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [28/Jan/2025 11:21:34] \"POST /predict HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [28/Jan/2025 11:21:36] \"POST /predict HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [28/Jan/2025 11:21:38] \"POST /predict HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [28/Jan/2025 11:21:41] \"POST /predict HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [28/Jan/2025 11:21:44] \"POST /predict HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [28/Jan/2025 11:23:00] \"POST /predict HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [28/Jan/2025 11:23:03] \"POST /predict HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [28/Jan/2025 11:23:11] \"POST /predict HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [28/Jan/2025 11:23:13] \"POST /predict HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [28/Jan/2025 11:23:15] \"POST /predict HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [28/Jan/2025 11:23:18] \"POST /predict HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [28/Jan/2025 11:23:21] \"POST /predict HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [28/Jan/2025 11:23:23] \"POST /predict HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [28/Jan/2025 11:28:38] \"POST /predict HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [28/Jan/2025 11:28:41] \"POST /predict HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [28/Jan/2025 11:28:44] \"POST /predict HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [28/Jan/2025 11:28:46] \"POST /predict HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [28/Jan/2025 11:28:47] \"POST /predict HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [28/Jan/2025 11:29:39] \"POST /predict HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [28/Jan/2025 11:29:41] \"POST /predict HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [28/Jan/2025 11:29:43] \"POST /predict HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [28/Jan/2025 11:29:45] \"POST /predict HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [28/Jan/2025 11:29:47] \"POST /predict HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [28/Jan/2025 11:29:50] \"POST /predict HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [28/Jan/2025 11:30:20] \"POST /predict HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [28/Jan/2025 11:30:22] \"POST /predict HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [28/Jan/2025 11:30:24] \"POST /predict HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [28/Jan/2025 11:30:26] \"POST /predict HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [28/Jan/2025 11:30:28] \"POST /predict HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [28/Jan/2025 11:30:31] \"POST /predict HTTP/1.1\" 200 -\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XoWP1bBHmJx7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}