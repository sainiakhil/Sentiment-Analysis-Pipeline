{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "from time import sleep"
      ],
      "metadata": {
        "id": "vmZrc8hMwMmM"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chat_with_bot(query, url=\"https://c6f3-34-125-53-18.ngrok-free.app/\"):\n",
        "    \"\"\"\n",
        "    Send a query to the chatbot and get response\n",
        "    \"\"\"\n",
        "    try:\n",
        "        response = requests.post(\n",
        "            f\"{url}/chat\",\n",
        "            json={\"query\": query}\n",
        "        )\n",
        "        return response.json()\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        return {\"error\": f\"Request failed: {str(e)}\"}\n",
        "\n",
        "def get_chat_history(url=\"https://c6f3-34-125-53-18.ngrok-free.app/\"):\n",
        "    \"\"\"\n",
        "    Retrieve chat history\n",
        "    \"\"\"\n",
        "    try:\n",
        "        response = requests.get(f\"{url}/history\")\n",
        "        return response.json()\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        return {\"error\": f\"Request failed: {str(e)}\"}\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Example chat\n",
        "    query = \"what is attention machnism in transformers\"\n",
        "    print(\"\\nSending query to chatbot...\")\n",
        "    response = chat_with_bot(query)\n",
        "    print(\"User query\", query)\n",
        "    print(\"Response:\", json.dumps(response, indent=2))\n",
        "\n",
        "    # Example get history\n",
        "    print(\"\\nGetting chat history...\")\n",
        "    history = get_chat_history()\n",
        "    print(\"History:\", json.dumps(history, indent=2))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j-pQXuXzx2mt",
        "outputId": "cb7a5dbd-48ee-48ea-ec16-6f9743d63259"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sending query to chatbot...\n",
            "User query what is attention machnism in transformers\n",
            "Response: {\n",
            "  \"message_id\": 1,\n",
            "  \"response\": \" In the Transformer architecture, the attention mechanism is a key component that enables the model to selectively focus on certain parts of the input sequence while ignoring others. This mechanism is responsible for capturing the relationships between tokens, allowing the model to learn and utilize long-range dependencies effectively. The attention mechanism works by comparing the input tokens to each other and assigning weights to them. These weights determine how much influence each token should have on the output. In the Transformer, this process involves transforming each token into a set of query (Q), key (K), and value (V) matrices. The Q and K matrices are then used to compute the attention scores,\"\n",
            "}\n",
            "\n",
            "Getting chat history...\n",
            "History: {\n",
            "  \"history\": [\n",
            "    {\n",
            "      \"User_query\": \"what is attention machnism in transformers\",\n",
            "      \"generated_response\": \" In the Transformer architecture, the attention mechanism is a key component that enables the model to selectively focus on certain parts of the input sequence while ignoring others. This mechanism is responsible for capturing the relationships between tokens, allowing the model to learn and utilize long-range dependencies effectively. The attention mechanism works by comparing the input tokens to each other and assigning weights to them. These weights determine how much influence each token should have on the output. In the Transformer, this process involves transforming each token into a set of query (Q), key (K), and value (V) matrices. The Q and K matrices are then used to compute the attention scores,\",\n",
            "      \"id\": 1,\n",
            "      \"role\": \"system\",\n",
            "      \"timestamp\": \"2025-01-30T06:19:26.970506\"\n",
            "    }\n",
            "  ]\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ADmQbh5Dz5PP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}